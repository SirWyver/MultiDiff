<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Rendering-guided 3D Radiance Field Diffusion">
    <meta name="keywords" content="DiffRF, Diffusion, 3D, Radiance Fields, NeRF, 3D Object">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MultiDiff: Consistent Novel View Synthesis from a Single Image</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="icon" href="static/method/diffrf_icon.gif" type="image/gif" />

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">MultiDiff: Consistent Novel View Synthesis from a
                            Single Image
                        </h1>
                        <h2 class="is-size-3">CVPR 2024
                        </h2>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://sirwyver.github.io">Norman
                                    Müller</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="https://katjaschwarz.github.io">Katja Schwarz</a><sup>1</sup>,</span>
                            <span class="author-block">
                                <a href="https://barbararoessle.github.io/">Barbara Roessle</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=vW1gaVEAAAAJ">Lorenzo
                                    Porzi</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?hl=de&user=484sccEAAAAJ">Samuel Rota
                                    Bulò</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://niessnerlab.org/members/matthias_niessner/profile.html">Matthias
                                    Nießner</a><sup>2</sup>
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=CxbDDRMAAAAJ&hl=en">Peter
                                    Kontschieder</a><sup>1</sup>,
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Meta Reality Labs</span>
                            <span class="author-block"><sup>2</sup>Technical University of Munich,</span>
                        </div>
                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="static/assets/MultiDiff.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2406.18524v1"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                                <!-- Video Link. -->
                                <span class="link-block">
                                    <a href="https://youtu.be/zBC4z4qXW_4"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <svg class="svg-inline--fa fa-youtube fa-w-18" aria-hidden="true"
                                                focusable="false" data-prefix="fab" data-icon="youtube" role="img"
                                                xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"
                                                data-fa-i2svg="">
                                                <path fill="currentColor"
                                                    d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z">
                                                </path>
                                            </svg>
                                        </span>
                                        <span>Video</span>
                                    </a>
                                </span>
                                <!-- Github Link. -->
                                <!-- <span class="link-block">
                                    <a class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                <span>Code (coming soon)</span>
                                </a>
                                </span> -->
                                <!-- Dataset Link. -->
                                <span class="link-block">
                            </div>
                        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">

                <video id="teaser" autoplay muted loop height="100%">
                    <source src="static/method/multidiff_teaser_vid.mp4" type="video/mp4">
                </video>
                <script>
                    var v = document.getElementById("teaser");
                    v.playbackRate = 1.5;

                </script>
                <h2 class=" subtitle has-text-centered">
                    <span class="dnerf">MultiDiff</span> enables camera-motion control for scene-level novel view synthesis. 
                    Given a single RGB image and a camera trajectory of choice, the model generates 3D-consistent views extrapolating from the input image. 
                </h2>


    </section>




    <!-- <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-steve">
                        <video poster="" id="steve" autoplay controls muted loop height="100%">
                            <source src="static/nuscenes/3edd8c1ef10b4f4e889e016cb56035e6.mp4" type="video/mp4">
                        </video>
                        <img src="static/nuscenes/3edd8c1ef10b4f4e889e016cb56035e6.jpg" />
                    </div>
                    <div class="item item-blueshirt">
                        <video poster="" id="blueshirt" autoplay controls muted loop height="100%">
                            <source src="static/nuscenes/ec7bb4009f5b42f6b497501b223ba8b2.mp4" type="video/mp4">
                        </video>
                        <img src="static/nuscenes/ec7bb4009f5b42f6b497501b223ba8b2.jpg" />
                    </div>
                    <div class="item item-fullbody">
                        <video poster="" id="fullbody" autoplay controls muted loop height="100%">
                            <source src="static/nuscenes/8166595e053d469ca5fcb0e12b45b068.mp4" type="video/mp4">
                        </video>
                        <img src="static/nuscenes/8166595e053d469ca5fcb0e12b45b068.jpg" />
                    </div>
                    <div class="item item-shiba">
                        <video poster="" id="shiba" autoplay controls muted loop height="100%">
                            <source src="static/nuscenes/82bfca2a2fad4589bce8acee7dd5d794.mp4" type="video/mp4">
                        </video>
                        <img src="static/nuscenes/82bfca2a2fad4589bce8acee7dd5d794.jpg" />
                    </div>
                    <div class="item item-chair-tp">
                        <video poster="" id="chair-tp" autoplay controls muted loop height="100%">
                            <source src="static/nuscenes/9adba99929b5432998dff5aefdfc0178.mp4" type="video/mp4">
                        </video>
                        <img src="static/nuscenes/9adba99929b5432998dff5aefdfc0178.jpg" />
                    </div>


                </div>
            </div>
        </div>
    </section> -->


    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        We introduce MultiDiff, a novel approach for consistent novel view synthesis of scenes from a
                        single RGB image.
                        The task of synthesizing novel views from a single reference image is highly ill-posed by
                        nature, as there exist multiple, plausible explanations for unobserved areas.
                        To address this issue, we incorporate strong priors in form of monocular depth predictors and
                        video-diffusion models. Monocular depth enables us to condition our model on warped reference
                        images for the target views, increasing geometric stability. The video-diffusion prior provides
                        a strong proxy for 3D scenes, allowing the model to learn continuous and pixel-accurate
                        correspondences across generated images.
                        In contrast to approaches relying on autoregressive image generation that are prone to drifts
                        and error accumulation, MultiDiff jointly synthesizes a sequence of frames yielding high-quality
                        and multi-view consistent results -- even for long-term scene generation with large camera
                        movements, while reducing inference time by an order of magnitude.
                        For additional consistency and image quality improvements, we introduce a novel, structured
                        noise distribution.
                        Our experimental results demonstrate that MultiDiff outperforms state-of-the-art methods on the
                        challenging, real-world datasets RealEstate10K and ScanNet.
                        Finally, our model naturally supports multi-view consistent editing without the need for further
                        tuning.
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
            <!-- Paper video. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Video</h2>
                    <div class=publication-video>
                        <iframe src="https://www.youtube.com/embed/zBC4z4qXW_4?si=sLFjNJWUXvPqzHKR"
                            title="YouTube video player"
                            frameborder="0"
                            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen></iframe>
                    </div>
                </div>
                </div>
            </div>
            <!--/ Paper video. -->
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Method</h2>

            <div class="content has-text-justified">
                <p>
                    <span class="dnerf">MultiDiff</span> leverages strong depth and video diffusion priors to enable
                        consistent novel view synthesis of scenes from a single RGB image using a novel correspondence attention layer.
                </p>
            </div>

                    <video id="method" autoplay muted loop height="100%">
                        <source src="static/method/method_cut.mp4" type="video/mp4">
                    </video>
                    <script>
                        var v = document.getElementById("method");
                        v.playbackRate = 8.0;

                    </script>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">

            <!-- Re-rendering. -->
            <div class="colums is-centered">
                <div class="column">
                    <div class="content">
                        <video id="results" autoplay muted loop>
                            <source src="static/results/comparisons_cut.mp4" type="video/mp4">
                        </video>
                        <script>
                            var v = document.getElementById("results");
                            v.playbackRate = 1.3;
                        </script>
                        <h4 class="  has-text-centered">
                            Novel-view rendering results following the GT trajectory.
                        </h4>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">

            <!-- Re-rendering. -->
            <div class="colums is-centered">
                <div class="column">
                    <div class="content">
                        <video id="results" autoplay muted loop>
                            <source src="static/method/noise_warping.mp4" type="video/mp4">
                        </video>
                        <script>
                            var v = document.getElementById("results");
                            v.playbackRate = 1.3;
                        </script>
                        
                        <h4 class="  has-text-centered">
                           By warping the initial noise according to the estimated depth into the target novel views, we can structure the noise providing additional information about the 3D scene structure. 
                           Just like Neo in "The Matrix", the model can decode this abstract noise pattern in more consistent views.
                        </h4>
                    </div>
                    
                </div>
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">

            <!-- Re-rendering. -->
            <div class="colums is-centered">
                <div class="column">
                    <div class="content">
                        <video id="results" autoplay muted loop>
                            <source src="static/results/editing.mp4" type="video/mp4">
                        </video>
                        <script>
                            var v = document.getElementById("results");
                            v.playbackRate = 1.3;
                        </script>

                        <h4 class="  has-text-centered">
                            By masking areas in the input image,  <span class="dnerf">MultiDiff</span> naturally enables consistent editing without the need for finetuning. 
                        </h4>


                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@InProceedings{Muller_2024_CVPR,
                author    = {M\"uller, Norman and Schwarz, Katja and R\"ossle, Barbara and Porzi, Lorenzo and Bul\`o, Samuel Rota and Nie{\ss}ner, Matthias and Kontschieder, Peter},
                title     = {MultiDiff: Consistent Novel View Synthesis from a Single Image},
                booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
                month     = {June},
                year      = {2024},
                pages     = {10258-10268}
            }
            </code>
        </pre>
        </div>
    </section>



    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="static/assets/MultiDiff.pdf">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/sirwyver" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p style="text-align:center">
                            Source code mainly borrowed from <a href="https://keunhong.com/">Keunhong Park</a>'s <a
                                href="https://nerfies.github.io/">Nerfies website</a>.
                        </p>
                        <p style="text-align:center">
                            Please contact <a href="https://sirwyver.github.io/">Norman
                                Müller</a> for feedback and questions.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>